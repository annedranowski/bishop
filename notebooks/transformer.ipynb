{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import torch\n",
    "\n",
    "BASE_DIR = Path(os.path.abspath(\"\")).parent\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "sys.path.insert(0, str(BASE_DIR / \"src\"))\n",
    "plot.style.use(\"dark_background\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from typing import Any\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as PILImage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class KnotCrossingCountDataset(VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str | Path,\n",
    "        transforms: Callable[[PILImage], Any] | None = None,\n",
    "        transform: Callable[[PILImage], Any] | None = None,\n",
    "        target_transform: Callable[[PILImage], Any] | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            root,\n",
    "            transforms,\n",
    "            transform,\n",
    "            target_transform,\n",
    "        )\n",
    "\n",
    "        classes, class_to_index = self.find_classes(self.root)\n",
    "        self.classes = classes\n",
    "        self.class_to_index = class_to_index\n",
    "\n",
    "        samples = self.find_samples(self.root, self.class_to_index)\n",
    "        self.samples = samples\n",
    "        self.targets = [target for _, target in samples]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        image_path, class_index = self.samples[index]\n",
    "        image = self.load_image(image_path)\n",
    "\n",
    "        if self.transforms is None:\n",
    "            return image, class_index\n",
    "\n",
    "        return self.transforms(image, class_index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def find_classes(self, root: str | Path) -> tuple[list[str], dict[str, int]]:\n",
    "        classes = sorted(\n",
    "            set(\n",
    "                self.get_class_from_dir_name(entry.name)\n",
    "                for entry in os.scandir(root)\n",
    "                if entry.is_dir()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        class_to_index = {class_: i for i, class_ in enumerate(classes)}\n",
    "\n",
    "        return classes, class_to_index\n",
    "\n",
    "    def find_samples(\n",
    "        self,\n",
    "        root: str | Path,\n",
    "        class_to_index: dict[str, int],\n",
    "    ) -> list[tuple[Path, int]]:\n",
    "        samples: list[tuple[Path, int]] = []\n",
    "\n",
    "        for dir in os.scandir(root):\n",
    "            if not dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            class_ = self.get_class_from_dir_name(dir.name)\n",
    "            class_index = class_to_index[class_]\n",
    "\n",
    "            for parent_dir, _, file_names in os.walk(dir.path):\n",
    "                for file_name in file_names:\n",
    "                    file_path = Path(parent_dir) / file_name\n",
    "\n",
    "                    if self.is_valid_file(file_path):\n",
    "                        samples.append((file_path, class_index))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def is_valid_file(self, path: Path) -> bool:\n",
    "        transforms_bitmask = int(path.stem.split(\"_\")[-2], 2)\n",
    "\n",
    "        return (transforms_bitmask & (1 << 3)) == 0 # No elastic transform\n",
    "\n",
    "    def load_image(self, path: Path) -> PILImage:\n",
    "        with open(path, \"rb\") as image_file:\n",
    "            return Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "    def get_class_from_dir_name(self, name: str) -> str:\n",
    "        return name.split(\"_\")[0]\n",
    "\n",
    "\n",
    "dataset = KnotCrossingCountDataset(\n",
    "    BASE_DIR / \"data\" / \"augmented\" / \"transformed_knots\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "split_indexes: list[list[int]] = train_test_split(\n",
    "    list(range(len(dataset))),\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=dataset.targets,\n",
    ")\n",
    "\n",
    "train_indexes, test_indexes = split_indexes\n",
    "\n",
    "train_set = Subset[tuple[Tensor, int]](dataset, train_indexes)\n",
    "test_set = Subset[tuple[Tensor, int]](dataset, test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_sample = train_set[random.randrange(len(train_set))]\n",
    "\n",
    "plot.imshow(random_sample[0].squeeze(0), cmap=\"grey\")\n",
    "plot.title(dataset.classes[random_sample[1]])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from modules import (\n",
    "    Conv2d,\n",
    "    Pad2dPropsWithSame,\n",
    "    EncoderLayer,\n",
    "    AttentionNoChannelsProps,\n",
    "    Conv2dNoChannelsFixedPaddingProps,\n",
    ")\n",
    "\n",
    "\n",
    "class Encoder(nn.Sequential):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                padding_props=Pad2dPropsWithSame(\n",
    "                    padding=\"same\",\n",
    "                ),\n",
    "            ),\n",
    "            EncoderLayer(\n",
    "                embedding_size=64,\n",
    "                attn_props=AttentionNoChannelsProps(\n",
    "                    heads=6,\n",
    "                    key_size=96,\n",
    "                    key_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    value_size=96,\n",
    "                    value_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    attn_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=7,\n",
    "                    ),\n",
    "                    out_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                ),\n",
    "                feedforward_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                    kernel_size=3,\n",
    "                ),\n",
    "            ),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "            ),\n",
    "            EncoderLayer(\n",
    "                embedding_size=64,\n",
    "                attn_props=AttentionNoChannelsProps(\n",
    "                    heads=6,\n",
    "                    key_size=96,\n",
    "                    key_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    value_size=96,\n",
    "                    value_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    attn_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=7,\n",
    "                    ),\n",
    "                    out_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                ),\n",
    "                feedforward_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                    kernel_size=3,\n",
    "                ),\n",
    "            ),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "            ),\n",
    "            EncoderLayer(\n",
    "                embedding_size=64,\n",
    "                attn_props=AttentionNoChannelsProps(\n",
    "                    heads=6,\n",
    "                    key_size=96,\n",
    "                    key_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    value_size=96,\n",
    "                    value_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    attn_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=7,\n",
    "                    ),\n",
    "                    out_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                ),\n",
    "                feedforward_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                    kernel_size=3,\n",
    "                ),\n",
    "            ),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "            ),\n",
    "            EncoderLayer(\n",
    "                embedding_size=64,\n",
    "                attn_props=AttentionNoChannelsProps(\n",
    "                    heads=6,\n",
    "                    key_size=96,\n",
    "                    key_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    value_size=96,\n",
    "                    value_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                    attn_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=7,\n",
    "                    ),\n",
    "                    out_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                        kernel_size=3,\n",
    "                    ),\n",
    "                ),\n",
    "                feedforward_conv_props=Conv2dNoChannelsFixedPaddingProps(\n",
    "                    kernel_size=3,\n",
    "                ),\n",
    "            ),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=64 * 4 * 4,\n",
    "                out_features=len(dataset.classes),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    loss_func: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = \"cpu\",\n",
    ") -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total_correct_count = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (x, y) in tqdm(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_logits: Tensor = model(x)\n",
    "        loss: Tensor = loss_func(y_logits, y)\n",
    "\n",
    "        total_loss += typing.cast(float, loss.item() * x.shape[0])\n",
    "        total_correct_count += (y_logits.argmax(dim=-1).long() == y).int().sum().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_set)\n",
    "    accuracy = total_correct_count / len(train_set)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_step(\n",
    "    model: nn.Module,\n",
    "    loss_func: nn.Module,\n",
    "    device: str = \"cpu\",\n",
    ") -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_correct_count = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for (x, y) in tqdm(test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_logits: Tensor = model(x)\n",
    "            loss: Tensor = loss_func(y_logits, y)\n",
    "\n",
    "            total_loss += typing.cast(float, loss.item() * x.shape[0])\n",
    "            total_correct_count += (y_logits.argmax(dim=-1).long() == y).int().sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_set)\n",
    "    accuracy = total_correct_count / len(test_set)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_step(model, loss_func, optimizer)\n",
    "    test_loss, test_acc = test_step(model, loss_func)\n",
    "\n",
    "    print(f\"epoch={epoch} train_loss={train_loss} train_acc={train_acc} test_loss={test_loss} test_acc={test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
